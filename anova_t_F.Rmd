---
title: "ANOVA: Concepts for t-based and F-based inference"
subtitle: "Sleuth3 Sections 6.2 and 5.2"
date: ""
output:
  pdf_document:
    fig_height: 2.8
    fig_width: 6
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
header-includes:
  - \usepackage{booktabs}
  - \usepackage{graphicx}
geometry: margin=0.6in
classoption: landscape
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

## The ANOVA Model

 * Observations in group $i$ follow a Normal($\mu_i$, $\sigma^2$) distribution
    * (Potentially) different mean for each group
    * Same variance across all groups
 * Note, this is just a generic model - it is not related to the iris data set featured in the remainder of these notes.

```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.height = 1.5}
library(ggplot2)
library(dplyr)
x_grid <- seq(from = 0, to = 20, length = 101)
group_sd <- 2
plot_df <- data.frame(
  x = rep(x_grid, 3),
  y = c(dnorm(x_grid, mean = 8, sd = group_sd),
        dnorm(x_grid, mean = 10, sd = group_sd),
        dnorm(x_grid, mean = 14, sd = group_sd)),
  group = rep(c("Group 1: mean = 8, sd = 2", "Group 2: mean = 10, sd = 2", "Group 3: mean = 14, sd = 2"),
    each = length(x_grid))
)
ggplot(data = plot_df, mapping = aes(x = x, y = y, color = group)) +
  geom_line()
```



## Notation

 * We have $I$ groups ($I = 3$ for iris example)
 * Sample size of $n_i$ for group $i$, total sample size $n = n_1 + n_2 + \cdots + n_I$
 * $y_{ij}$: response variable value for unit $j$ in group $i$
     * $i$: which group? ($i$ = 1, 2, or 3 for iris flowers since there are $I = 3$ species)
     * $j$: which observational unit within its group?  (if $i = 2$ and $j = 3$, we're talking about the 3rd versicolor flower)
 * $\bar{y}_i$: sample mean for group $i$

## Two Types of Hypotheses:

1. $H_0: C_1 \mu_1 + C_2 \mu_2 + \cdots + C_I \mu_I = 0$.
    * Some combination of means is equal to 0
    * Can specify with one = sign
    * Use a t test

2. $H_0: \mu_1 = \mu_2 = \mu_3$.
    * Some of the group means are actually equal to each other
    * Need multiple = signs to specify
    * Use an F test

<!-- \newpage -->

## t statistic for ANOVA model

\includegraphics{t_stat.pdf}

\newpage

## F tests

Suppose we are conducting a test of $H_0: \mu_{1} = \mu_{2} = \mu_{3}$ vs. $H_A:$ at least one of the means differs from the others

We frame this as a comparison of two models.

#### 1. Full Model, separate means for all groups (corresponds to $H_A$)

3 mean parameters: $\mu_1$, $\mu_2$, $\mu_3$

```{r, fig.height = 1.5, echo = FALSE, fig.width = 5.8}
# Calculate sample means and standard deviations separately for each species
group_means <- iris %>%
  group_by(Species) %>%
  summarize(
    mean = mean(Sepal.Width)
  )
ggplot(data = iris, mapping = aes(x = Sepal.Width, color = Species)) +
  geom_density() +
  geom_vline(data = group_means, mapping = aes(xintercept = mean, color = Species)) +
  xlim(c(1, 5))
```

#### 2. Reduced Model, one mean common to all observations (corresponds to $H_0$)

1 mean parameter: $\mu$

```{r, fig.height = 1.5, fig.width = 4.8, echo = FALSE}
# Calculate overall sample mean and standard deviation
overall_mean <- iris %>%
  summarize(
    mean = mean(Sepal.Width),
    sd = sd(Sepal.Width)
  )
ggplot(data = iris, mapping = aes(x = Sepal.Width)) +
  geom_density() +
  geom_vline(xintercept = overall_mean$mean) +
  xlim(c(1, 5))
```

#### How should we measure the usefulness of a model?

 * Suppose we know a flower is from the setosa species, and we want to guess its sepal width.  Which guess is better?  Why?  Can you think of a quantitative way to explain?
    * The group mean for setosa flowers, $\bar{Y}_1$.  Location of red line in top plot, about 3.5
    * The overall mean for iris flowers, $\bar{Y}$.  Location of black line in lower plot, about 3.

\newpage

#### Residuals

 * **Residual**: difference between observed value for response variable and fitted value for response variable.

$$res_{ij} = Y_{ij} - \bar{Y}_i$$

 * In general: $\qquad \text{Better Model} \Leftrightarrow \text{Better Guesses} \Leftrightarrow \text{Smaller Residuals}$
 * The Full Model will have smaller residuals (on average) than the Reduced Model
 * F test answers: are the residuals from the full model enough smaller than the residuals from the reduced model that I think the full model is necessary?

#### Measuring the size of residuals from a model

 * Residual Sum of Squares: Square the residuals and add them up
$$\sum_i \sum_j (res_{ij})^2 = \sum_i \sum_j (Y_{ij} - \bar{Y}_i)^2$$
 * Mean Squared Residual:
$$\frac{\text{Residual Sum of Squares}}{\text{Degrees of Freedom}}$$

#### Example

Suppose I have just 3 flowers of each species.  Below is an example of the calculation of the RSS for the reduced model (one mean) and the full model (separate means for each group).

```{r, eval = FALSE, echo = FALSE}
set.seed(1234)
table_vals <- iris %>%
  group_by(Species) %>%
  sample_n(size = 3) %>%
  select(Species, Sepal.Width) %>%
  mutate(
    j = row_number(),
    full_mean = round(mean(Sepal.Width), 3),
    full_resid = Sepal.Width - full_mean,
    full_resid_sq = round(full_resid^2, 3)
  ) %>%
  ungroup() %>%
  mutate(
    i = rep(1:3, each = 3),
    Species = as.character(Species),
    reduced_mean = round(mean(Sepal.Width), 3),
    reduced_resid = Sepal.Width - reduced_mean,
    reduced_resid_sq = round(reduced_resid^2, 3)
  ) %>%
  select(
    i, j, Species, Sepal.Width, reduced_mean, reduced_resid, reduced_resid_sq, full_mean, full_resid, full_resid_sq
  )
for(i in seq_len(nrow(table_vals))) {
  cat(paste(as.character(table_vals[i, ]), collapse = " & "))
  cat("\\\\\n\\midrule\n")
}
sum(table_vals$full_resid_sq)
sum(table_vals$reduced_resid_sq)
```


\begin{table}[!hb]
\centering
\begin{tabular}{ccccccccccc}
\toprule
 & & & & \multicolumn{3}{c}{Reduced Model} & & \multicolumn{3}{c}{Full Model} \\
\cline{5-7} \cline{9-11} \\
i & j & Species & Sepal Width ($Y_{ij}$) & Mean & Residual & Squared Residual & & Mean & Residual & Squared Residual \\
\midrule
1 & 1 & setosa & 3.9 & 3.044 & 0.856 & 0.733 & & 3.4 & 0.5 & 0.25\\
\midrule
1 & 2 & setosa & 3.1 & 3.044 & 0.056 & 0.003 & & 3.4 & -0.3 & 0.09\\
\midrule
1 & 3 & setosa & 3.2 & 3.044 & 0.156 & 0.024 & & 3.4 & -0.2 & 0.04\\
\midrule
2 & 1 & versicolor & 2.4 & 3.044 & -0.644 & 0.415 & & 2.467 & -0.067 & 0.004\\
\midrule
2 & 2 & versicolor & 2.6 & 3.044 & -0.444 & 0.197 & & 2.467 & 0.133 & 0.018\\
\midrule
2 & 3 & versicolor & 2.4 & 3.044 & -0.644 & 0.415 & & 2.467 & -0.067 & 0.004\\
\midrule
3 & 1 & virginica & 3.3 & 3.044 & 0.256 & 0.066 & & 3.267 & 0.033 & 0.001\\
\midrule
3 & 2 & virginica & 2.7 & 3.044 & -0.344 & 0.118 & & 3.267 & -0.567 & 0.321\\
\midrule
3 & 3 & virginica & 3.8 & 3.044 & 0.756 & 0.572 & & 3.267 & 0.533 & 0.284\\
\bottomrule
Total & & & & & & 2.543 & & & & 1.012 \\
\bottomrule
\end{tabular}
\end{table}


#### Extra Sum of Squares

\begin{align*}
\text{Extra Sum of Squares} &= \text{Residual Sum of Squares, Reduced Model} - \text{Residual Sum of Squares, Full Model} \\
&= 2.543 - 1.012 \\
&= 1.531
\end{align*}

 * Always positive because
    * Reduced Model is more limited than Full Model
    * Reduced Model has larger residuals than Full Model

 * If Extra Sum of Squares is really big, the Full Model is much better than the Reduced Model

 * You can calculate the degrees of freedom for the Extra Sum of Squares in either of two ways:
    * difference in degrees of freedom for the full model and the reduced model: $(n - 1) - (n - I) = I - 1 = 3 - 1 = 2$
    * difference in number of parameters for the mean between full and reduced model: $3 - 1 = 2$

#### F Statistic

 * "How big is the improvement in Residual Sum of Squares from using the Full Model instead of the Reduced Model"?
    * Size of improvement is measured relative to the size of residuals in the full model

\begin{align*}
F &= \frac{(\text{Extra Sum of Squares})/(\text{Extra Degrees of Freedom})}{(\text{Residual Sum of Squares, Full Model})/(\text{Degrees of Freedom, Full Model})} \\
 &= \frac{1.531/(3 - 1)}{1.012/(9 - 3)} \\
 &= 4.539
\end{align*}

 * If $H_{0}: \mu_1 = \mu_2 = \mu_3$ is **true**, then...
    * Full Model **isn't better** than Reduced Model
    * Residual Sum of Squares, Full Model is **similar to** Residual Sum of Squares, Reduced Model
    * Extra Sum of Squares is **small**
    * F Statistic is **small**

 * If $H_O: \mu_1 = \mu_2 = \mu_3$ is **not true**, then...
    * Full Model **is better** than Reduced Model
    * Residual Sum of Squares, Full Model is **smaller than** Residual Sum of Squares, Reduced Model
    * Extra Sum of Squares is **large**
    * F Statistic is **large**

 * **A large value of F statistic is evidence against** $H_0$
 
 * For finding p-values we are interested in the probability of getting an F statistic at least as large as the F statistic we got from our sample, if $H_0$ is true.

```{r, echo = FALSE, message=FALSE, cache = TRUE, fig.height = 2}
fval <- 4.539
plot_ub <- 8
x_grid_2 <- seq(from = fval, to = plot_ub, length = 101)
region_to_shade2 <- data.frame(
  x = c(fval, x_grid_2, plot_ub),
  y = c(0, df(x_grid_2, df1 = 2, df2 = 6, log = FALSE), 0)
)
ggplot(data = data.frame(x = c(0, plot_ub)), mapping = aes(x = x)) +
  stat_function(fun = df, args = list(df1 = 2, df2 = 6)) +
  geom_polygon(
    mapping = aes(x = x, y = y),
    fill = "cornflowerblue",
    data = region_to_shade2) +
  geom_vline(xintercept = fval) +
  coord_cartesian(xlim = c(0, plot_ub), expand = FALSE) +
  scale_x_continuous(breaks = c(0, fval), labels = c("0", fval)) +
  xlab("F") +
  ylab("") +
  theme_bw()
```

 * We have to keep track of two degrees of freedom
